ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| idx: 1, rank_idx: 0
[rank1]: Traceback (most recent call last):
[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 896, in <module>
[rank1]:     test_tp_forward(4, replicate_dataset=True)
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 458, in test_tp_forward
[rank1]:     ddp_out = forward_pass2(ddp_trainer)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 440, in forward_pass2
[rank1]:     batch = next(next(iter(trainer.state.train_dataloader)))
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: TypeError: 'list' object is not an iterator
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| idx: 1, rank_idx: 0
[rank1]: Traceback (most recent call last):
[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 898, in <module>
[rank1]:     test_tp_forward(4, replicate_dataset=True)
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 460, in test_tp_forward
[rank1]:     ddp_out = forward_pass2(ddp_trainer)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 442, in forward_pass2
[rank1]:     batch = next(0)
[rank1]:             ^^^^^^^
[rank1]: TypeError: 'int' object is not an iterator
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| idx: 1, rank_idx: 0
ic| idx: 5, rank_idx: 1
ic| inputs: tensor([[-1.4643,  0.8915]], device='cuda:1')
    outputs: tensor([[-0.2457, -0.3233]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| idx: 1, rank_idx: 0
ic| idx: 5, rank_idx: 1
ic| inputs: tensor([[-1.4643,  0.8915]], device='cuda:1')
    outputs: tensor([[-0.2457, -0.3233]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683],
                    [ 0.5177, -0.3069],
                    [-0.4842, -1.4171],
                    [-0.0338, -1.7419],
                    [ 0.2120, -0.9496]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| idx: 1, rank_idx: 0
ic| idx: 5, rank_idx: 1
ic| inputs: tensor([[-1.4643,  0.8915]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.2457, -0.3233]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| dist.get_world_size(): 4, dist.get_local_world_size(): 4
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| dist.get_world_size(): 4, dist.get_local_world_size(): 4
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| dist.get_world_size(): 4, dist.get_local_world_size(): 4
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| idx: 1, rank_idx: 0
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[-0.1163,  1.2457],
                    [ 1.6768, -0.3361],
                    [-0.1486, -1.4091],
                    [-2.0410, -0.2618]], device='cuda:1')
ic| self.y: tensor([0, 1, 1, 1], device='cuda:1')
ic| inputs: tensor([[-0.1163,  1.2457]], device='cuda:1')
    outputs: tensor([[-0.0748, -0.0189]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[-0.1163,  1.2457],
                    [ 1.6768, -0.3361],
                    [-0.1486, -1.4091],
                    [-2.0410, -0.2618]], device='cuda:1')
ic| self.y: tensor([0, 1, 1, 1], device='cuda:1')
ic| inputs: tensor([[-0.1163,  1.2457]], device='cuda:1')
    outputs: tensor([[-0.0748, -0.0189]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[-0.1163,  1.2457],
                    [ 1.6768, -0.3361],
                    [-0.1486, -1.4091],
                    [-2.0410, -0.2618]], device='cuda:1')
ic| self.y: tensor([0, 1, 1, 1], device='cuda:1')
ic| inputs: tensor([[-0.1163,  1.2457]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0637, -0.0169]], device='cuda:1'))
[rank1]: Traceback (most recent call last):
[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 805, in <module>
[rank1]:     test_tp_forward(4, replication=True)
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 387, in test_tp_forward
[rank1]:     torch.testing.assert_close(
[rank1]:   File "/root/tp/.venv/tp-1/lib/python3.11/site-packages/torch/testing/_comparison.py", line 1524, in assert_close
[rank1]:     raise error_metas[0].to_error(msg)
[rank1]: AssertionError: DDP and TP-FSDP outputs from the forward pass are not close enough:
[rank1]: ddp_out=tensor([[-0.0748, -0.0189]], device='cuda:1', grad_fn=<MmBackward0>)
[rank1]: tp_fsdp_out=AsyncCollectiveTensor(tensor([[-0.0637, -0.0169]], device='cuda:1')).
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| self.x: tensor([[ 0.4857, -0.0272],
                    [-1.4643,  0.8915],
                    [ 1.5350,  0.0342],
                    [ 0.1812, -0.5683]], device='cuda:1')
ic| self.y: tensor([0, 0, 1, 0], device='cuda:1')
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| 'ddp'
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'fsdp'
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| 'tp_fsdp'
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
[rank1]:[W918 19:30:16.109758802 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.4257,  0.0528],
                            [ 0.0000,  0.0000]], device='cuda:1')
ic| rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1134, 0.0000],
                            [0.1134, 0.0000]], device='cuda:1')
[rank1]:[W918 19:32:25.765823594 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| rank: 1
[rank1]: Traceback (most recent call last):
[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 807, in <module>
[rank1]:     test_tp_gradients(4)
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 515, in test_tp_gradients
[rank1]:     ic('DDP', ddp_name, ddp_param.shape, ddp_param.weight)
[rank1]:                                          ^^^^^^^^^^^^^^^^
[rank1]: AttributeError: 'Parameter' object has no attribute 'weight'
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.4257,  0.0528],
                            [ 0.0000,  0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1134, 0.0000],
                            [0.1134, 0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    fsdp_param.shape: torch.Size([0])
    fsdp_param: Parameter containing:
                tensor([], device='cuda:1', requires_grad=True)
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    tp_fsdp_param.shape: torch.Size([0])
    tp_fsdp_param: Parameter containing:
                   tensor([], device='cuda:1', requires_grad=True)
[rank1]:[W918 19:35:59.291569221 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.4257,  0.0528],
                            [ 0.0000,  0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1134, 0.0000],
                            [0.1134, 0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    fsdp_param.shape: torch.Size([0])
    fsdp_param: Parameter containing:
                tensor([], device='cuda:1', requires_grad=True)
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    tp_fsdp_param.shape: torch.Size([0])
    tp_fsdp_param: Parameter containing:
                   tensor([], device='cuda:1', requires_grad=True)
[rank1]:[W918 19:37:19.573594581 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.4257,  0.0528],
                            [ 0.0000,  0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1134, 0.0000],
                            [0.1134, 0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    fsdp_param.shape: torch.Size([0])
    fsdp_param: Parameter containing:
                tensor([], device='cuda:1', requires_grad=True)
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    tp_fsdp_param.shape: torch.Size([0])
    tp_fsdp_param: Parameter containing:
                   tensor([], device='cuda:1', requires_grad=True)
[rank1]:[W918 19:38:10.583266334 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.4257,  0.0528],
                            [ 0.0000,  0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1134, 0.0000],
                            [0.1134, 0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    fsdp_param.shape: torch.Size([0])
    fsdp_param: Parameter containing:
                tensor([], device='cuda:1', requires_grad=True)
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    tp_fsdp_param.shape: torch.Size([0])
    tp_fsdp_param: Parameter containing:
                   tensor([], device='cuda:1', requires_grad=True)
[rank1]:[W918 19:39:48.117620806 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Exception ignored in: <module 'threading' from '/usr/lib/python3.11/threading.py'>
Traceback (most recent call last):
  File "/usr/lib/python3.11/threading.py", line 1541, in _shutdown
    def _shutdown():
    
  File "/root/tp/composer/composer/core/engine.py", line 123, in sigterm_handler
    sys.exit(128 + signal)
SystemExit: 143
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: tensor([[-0.1876, -0.0342]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 1.6768, -0.3361]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.1876, -0.0342]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.4257,  0.0528],
                            [ 0.0000,  0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([0., 0.], device='cuda:1')
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1134, 0.0000],
                            [0.1134, 0.0000]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    fsdp_param.shape: torch.Size([0])
    fsdp_param: Parameter containing:
                tensor([], device='cuda:1', requires_grad=True)
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    tp_fsdp_param.shape: torch.Size([0])
    tp_fsdp_param: Parameter containing:
                   tensor([], device='cuda:1', requires_grad=True)
[rank1]:[W918 19:41:32.709106174 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.1332, -0.4391],
                            [ 0.0664, -0.7116]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([ 0.0664, -0.7116], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([ 0.0664, -0.7116], device='cuda:1')
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.2.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[-0.6096, -0.4934],
                       [-0.1113, -0.6492]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[0.1061, 0.0047],
                            [0.1061, 0.0047]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    fsdp_param.shape: torch.Size([0])
    fsdp_param: Parameter containing:
                tensor([], device='cuda:1', requires_grad=True)
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.2.weight'
    tp_fsdp_param.shape: torch.Size([0])
    tp_fsdp_param: Parameter containing:
                   tensor([], device='cuda:1', requires_grad=True)
[rank1]:[W918 19:42:44.559180751 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: tensor([[-0.0591, -0.0108]], device='cuda:1', grad_fn=<MmBackward0>)
ic| inputs: tensor([[ 0.4857, -0.0272]], device='cuda:1')
    outputs: AsyncCollectiveTensor(tensor([[-0.0591, -0.0108]], device='cuda:1'))
ic| '-'*70: '----------------------------------------------------------------------'
    rank: 1
ic| 'DDP': 'DDP'
    ddp_name: 'module.module.0.weight'
    ddp_param.shape: torch.Size([2, 2])
    ddp_param: Parameter containing:
               tensor([[ 0.2059,  0.1117],
                       [-0.3558, -0.0257]], device='cuda:1', requires_grad=True)
ic| ddp_param.grad.shape: torch.Size([2, 2])
    ddp_param.grad: tensor([[-0.1332, -0.4391],
                            [ 0.0664, -0.7116]], device='cuda:1')
ic| 'FSDP': 'FSDP'
    fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    fsdp_param.shape: torch.Size([2])
    fsdp_param: Parameter containing:
                tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| fsdp_param.grad.shape: torch.Size([2])
    fsdp_param.grad: tensor([ 0.0664, -0.7116], device='cuda:1')
ic| 'TP-FSDP': 'TP-FSDP'
    tp_fsdp_name: 'module._fsdp_wrapped_module.0.weight'
    tp_fsdp_param.shape: torch.Size([2])
    tp_fsdp_param: Parameter containing:
                   tensor([-0.3558, -0.0257], device='cuda:1', requires_grad=True)
ic| tp_fsdp_param.grad.shape: torch.Size([2])
    tp_fsdp_param.grad: tensor([ 0.0664, -0.7116], device='cuda:1')
[rank1]: Traceback (most recent call last):
[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 805, in <module>
[rank1]:     test_tp_gradients(4, replication=2)
[rank1]:   File "/root/tp/composer/tests/trainer/test_tp.py", line 521, in test_tp_gradients
[rank1]:     torch.testing.assert_close(
[rank1]:   File "/root/tp/.venv/tp-1/lib/python3.11/site-packages/torch/testing/_comparison.py", line 1524, in assert_close
[rank1]:     raise error_metas[0].to_error(msg)
[rank1]: AssertionError: DDP and FSDP gradients are not close enough:
[rank1]: ddp_param.grad=tensor([[-0.1332, -0.4391],
[rank1]:         [ 0.0664, -0.7116]], device='cuda:1')
[rank1]: fsdp_param.grad=tensor([ 0.0664, -0.7116], device='cuda:1')
